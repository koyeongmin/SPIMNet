{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral-invariant Matching Network\n",
    "\n",
    "## The source code  contains\n",
    "\n",
    " - Data preparation\n",
    " - Code description\n",
    "\n",
    "## Dataset preparation\n",
    "\n",
    "1) Downloading three cross-spectral and multi-spectral dataset\n",
    "\n",
    "RGB-NIR Scene Dataset\n",
    "\n",
    "https://ivrlwww.epfl.ch/supplementary_material/cvpr11/nirscene1.zip\n",
    "\n",
    "PittsStereo-RGBNIR: A Large RGB-NIR Stereo Dataset Collected in Pittsburgh with Challenging Materials\n",
    "\n",
    "http://www.cs.cmu.edu/~ILIM/projects/AA/RGBNIRStereo/\n",
    "\n",
    "KAIST Multispectral Pedestrian Dataset\n",
    "\n",
    "https://sites.google.com/site/pedestrianbenchmark/\n",
    "\n",
    "2) Use the 'ICCV2021_Data Preparation for RGB-NIR Patch Dataset.ipynb', 'ICCV2021_Data Preparation for RGB-NIR Stereo Dataset.ipynb', and 'ICCV2021_Data Preparation for KAIST RGB-thermal Dataset.ipynb' files to construct image patch datasets.\n",
    "\n",
    "\n",
    "## Train\n",
    "\n",
    "1) Training setting\n",
    "\n",
    "We trained our model from scratch for 35 epochs in total. All the convolution and convolution transpose layers used the initialisation method in [15] to set initial values for their weights. All models were trained in an end-to-end manner with the ADAM optimiser (β1 = 0.9, β2 = 0.999) [23]. We used a batch size of 32 and set the learning rate to 0.04 with a decay factor 0.1 after 20 epochs. The training was performed with a customised version of Tensorﬂow 2.0 on an NVIDIA Titan Xp GPU, which usually takes two days. A forward pass of SPIMNet takes about 0.1 seconds for patches with a 64×64 resolution. \n",
    "\n",
    "To prevent an overﬁtting problem, all samples were normalised to [-1, 1], and data augmentation was carried out through random ﬂipping, random rotation (90, 180, 270 degrees), and random cropping. In addition, two regularisation techniques were employed: the label smoothing [39] and L2 kernel regularisation for the convolution layers of the feature extraction networks with l2 = 0.001.\n",
    "\n",
    "2) Code description\n",
    "2.1) Important packages\n",
    "     - sklearn\n",
    "     - scipy\n",
    "     - tensorflow\n",
    "     - tensorflow_addons\n",
    "     \n",
    " 2.2) Important functions\n",
    "    - load(image_file): Read an image based on its path using Tensorflow\n",
    "    - normalize(): Normalize pixel values to the range [-1,1]\n",
    "    - Data augmentation including\n",
    "      + resize()\n",
    "      + random_crop()\n",
    "      + random_jitter(): flip images, rotate images, change brightness and contrast\n",
    "    - downsample(): create a block for an encoding network\n",
    "    - upsample(): create a block for a decoding network\n",
    "    - extract_features(): create a block of an extraction network\n",
    "    - RGB2NIR_convertor(): create VIS2NIR network\n",
    "    - NIR2RGB_convertor(): create NIR2VIS network\n",
    "    - NIR_domain_matching(): Feature extraction in NIR domain\n",
    "    - RGB_domain_matching(): Feature extraction in VIS domain\n",
    "    - similaritor_loss() loss function for SPIMNet\n",
    "    - train(): train SPIMNet\n",
    "    - compute_test_acc(): compute FPR95 for the testing sets\n",
    "    \n",
    "2.3) We provide jupyter files each of which divided into cells of codes. Each cell is designed for a small task or a function, and we have added comments for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:43: UserWarning: You are currently using a nightly version of TensorFlow (2.7.0-dev20210627). \n",
      "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
      "If you encounter a bug, do not file an issue on GitHub.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.7.0-dev20210627'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 1: Package declaration \n",
    "\n",
    "#%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "#%env CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from IPython import display\n",
    "from sklearn import metrics\n",
    "from scipy import interpolate\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: load images using Tensorflow\n",
    "\n",
    "PATH = '/home/shared_dir/research/SPIMNet/NIR_RGB/patch-merged-datasets/'\n",
    "\n",
    "BUFFER_SIZE = 1024*4\n",
    "BATCH_SIZE  = 8  # for each positive and negative pairs, altogether = 32\n",
    "IMG_WIDTH   = 64\n",
    "IMG_HEIGHT  = 64\n",
    "n_train_samples = 138752\n",
    "\n",
    "def load(image_file):\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "\n",
    "    w = tf.shape(image)[1]\n",
    "    w = w // 4\n",
    "    \n",
    "    rgb_pos = image[:, :w, :]\n",
    "    nir_pos = image[:, w*1:w*2, :]\n",
    "    rgb_neg = image[:, w*2:w*3, :]\n",
    "    nir_neg = image[:, w*3:w*4, :]\n",
    "\n",
    "    rgb_pos = tf.cast(rgb_pos, tf.float32)\n",
    "    nir_pos = tf.cast(nir_pos, tf.float32)\n",
    "    rgb_neg = tf.cast(rgb_neg, tf.float32)\n",
    "    nir_neg = tf.cast(nir_neg, tf.float32)\n",
    "\n",
    "    return rgb_pos, nir_pos, rgb_neg, nir_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 3: data augmentation\n",
    "\n",
    "def resize(input_l, input_r, target_l, target_r, height, width):\n",
    "    input_l  = tf.image.resize(input_l, [height, width],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    input_r  = tf.image.resize(input_r, [height, width],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    target_l = tf.image.resize(target_l, [height, width],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    target_r = tf.image.resize(target_r, [height, width],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    return input_l, input_r, target_l, target_r\n",
    "\n",
    "def random_crop(input_l, input_r, target_l, target_r):\n",
    "    stacked_image = tf.stack([input_l, input_r, target_l, target_r], axis=0)\n",
    "    cropped_image = tf.image.random_crop(stacked_image, size=[4, IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "\n",
    "    return cropped_image[0], cropped_image[1], cropped_image[2], cropped_image[3]\n",
    "\n",
    "# normalizing the images to [-1, 1]\n",
    "def normalize(input_l, input_r, target_l, target_r):\n",
    "    input_l  = (input_l / 127.5) - 1\n",
    "    input_r  = (input_r / 127.5) - 1\n",
    "    target_l = (target_l / 127.5) - 1\n",
    "    target_r = (target_r / 127.5) - 1\n",
    "\n",
    "    return input_l, input_r, target_l, target_r\n",
    "\n",
    "def random_jitter(input_l, input_r, target_l, target_r):    \n",
    "    # resize to 68x68\n",
    "    input_l, input_r, target_l, target_r = resize(input_l, input_r, target_l, target_r, 68, 68)\n",
    "    \n",
    "    # crop\n",
    "    input_l, input_r, target_l, target_r = random_crop(input_l, input_r, target_l, target_r)\n",
    "\n",
    "    # flip_left_right\n",
    "    if tf.random.uniform(()) > 0.5:        \n",
    "        input_l  = tf.image.flip_left_right(input_l)\n",
    "        input_r  = tf.image.flip_left_right(input_r)\n",
    "        target_l = tf.image.flip_left_right(target_l)\n",
    "        target_r = tf.image.flip_left_right(target_r)\n",
    "        \n",
    "    # flip_up_down\n",
    "    if tf.random.uniform(()) > 0.5:        \n",
    "        input_l  = tf.image.flip_up_down(input_l)\n",
    "        input_r  = tf.image.flip_up_down(input_r)\n",
    "        target_l = tf.image.flip_up_down(target_l)\n",
    "        target_r = tf.image.flip_up_down(target_r)\n",
    "        \n",
    "    # brighness change\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        rand_value  = tf.random.uniform((), minval=-5.0, maxval=5.0)        \n",
    "        input_l = input_l + rand_value\n",
    "        \n",
    "        rand_value  = tf.random.uniform((), minval=-5.0, maxval=5.0)\n",
    "        input_r = input_r + rand_value\n",
    "        \n",
    "        rand_value  = tf.random.uniform((), minval=-5.0, maxval=5.0)        \n",
    "        target_l = target_l + rand_value\n",
    "        \n",
    "        rand_value  = tf.random.uniform((), minval=-5.0, maxval=5.0)\n",
    "        target_r = target_r + rand_value\n",
    "                 \n",
    "    # contrast change\n",
    "    if tf.random.uniform(()) > 0.5:        \n",
    "        rand_value = tf.random.uniform((), minval=0.8, maxval=1.2)\n",
    "        mean_value = tf.reduce_mean(input_l)\n",
    "        input_l   = (input_l - mean_value)*rand_value + mean_value\n",
    "        \n",
    "        rand_value = tf.random.uniform((), minval=0.8, maxval=1.2)\n",
    "        mean_value = tf.reduce_mean(input_r)\n",
    "        input_r   = (input_r - mean_value)*rand_value + mean_value\n",
    "        \n",
    "        rand_value = tf.random.uniform((), minval=0.8, maxval=1.2)\n",
    "        mean_value = tf.reduce_mean(target_l)\n",
    "        target_l   = (target_l - mean_value)*rand_value + mean_value\n",
    "        \n",
    "        rand_value = tf.random.uniform((), minval=0.8, maxval=1.2)\n",
    "        mean_value = tf.reduce_mean(target_r)\n",
    "        target_r   = (target_r - mean_value)*rand_value + mean_value\n",
    "\n",
    "    \n",
    "    # clip value\n",
    "    input_l  = tf.clip_by_value(input_l, clip_value_min=0.0, clip_value_max=255.0)\n",
    "    input_r  = tf.clip_by_value(input_r, clip_value_min=0.0, clip_value_max=255.0)\n",
    "    target_l = tf.clip_by_value(target_l, clip_value_min=0.0, clip_value_max=255.0)\n",
    "    target_r = tf.clip_by_value(target_r, clip_value_min=0.0, clip_value_max=255.0)          \n",
    "    \n",
    "    # rotate positive samples for making hard positive cases\n",
    "    if tf.random.uniform(()) > 0.5: \n",
    "        if tf.random.uniform(()) < 0.5:\n",
    "            input_l = tfa.image.rotate(input_l, 1.5707963268) # 90\n",
    "            input_r = tfa.image.rotate(input_r, 1.570796326)  # 90\n",
    "        else:\n",
    "            input_l = tfa.image.rotate(input_l, 4.7123889804) # 270\n",
    "            input_r = tfa.image.rotate(input_r, 4.7123889804) # 270\n",
    "                \n",
    "    return input_l, input_r, target_l, target_r\n",
    "\n",
    "def load_image_train(image_file):\n",
    "    input_l, input_r, target_l, target_r = load(image_file)\n",
    "    input_l, input_r, target_l, target_r = random_jitter(input_l, input_r, target_l, target_r)\n",
    "    input_l, input_r, target_l, target_r = normalize(input_l, input_r, target_l, target_r)\n",
    "\n",
    "    return input_l, input_r, target_l, target_r\n",
    "\n",
    "def load_image_test(image_file):\n",
    "    input_l, input_r, target_l, target_r = load(image_file)\n",
    "    input_l, input_r, target_l, target_r = resize(input_l, input_r, target_l, target_r, IMG_HEIGHT, IMG_WIDTH)\n",
    "    input_l, input_r, target_l, target_r = normalize(input_l, input_r, target_l, target_r)\n",
    "\n",
    "    return input_l, input_r, target_l, target_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 4: load training data\n",
    "\n",
    "# train_dataset\n",
    "train_dataset = tf.data.Dataset.list_files(PATH+'country/*.jpg')\n",
    "train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 5: Network building blocks\n",
    "\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.keras.initializers.he_normal(seed=None)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "        result.add(tfa.layers.InstanceNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "def upsample(filters, size):\n",
    "    initializer = tf.keras.initializers.he_normal(seed=None)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n",
    "                                    kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "    result.add(tfa.layers.InstanceNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "def extract_first_features(filters, size, strides, apply_batchnorm=True):\n",
    "    initializer = tf.keras.initializers.he_normal(seed=None)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(tf.keras.layers.Conv2D(filters, size, strides=strides, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False, \n",
    "                             kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "        result.add(tfa.layers.InstanceNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 6: RGB2NIR network\n",
    "\n",
    "def RGB2NIR_convertor(input_x):\n",
    "    # input shape:  (64, 64, 3)\n",
    "    # output shape: (64, 64, 1)\n",
    "    \n",
    "    x_1 = input_x\n",
    "    \n",
    "    down_stack = [\n",
    "        downsample(64, 4,  apply_batchnorm=False), # (bs, 32, 32, 64)\n",
    "        downsample(128, 4, apply_batchnorm=True),  # (bs, 16, 16, 512)\n",
    "        downsample(256, 4, apply_batchnorm=True),  # (bs, 8, 8, 512)\n",
    "        downsample(256, 4, apply_batchnorm=True),  # (bs, 4, 4, 512)\n",
    "        downsample(256, 4, apply_batchnorm=True),  # (bs, 2, 2, 512)\n",
    "        downsample(256, 4, apply_batchnorm=True),  # (bs, 1, 1, 512)\n",
    "    ]\n",
    "\n",
    "    up_stack = [\n",
    "        upsample(256, 4), # (bs, 2, 2, 1024)\n",
    "        upsample(256, 4), # (bs, 4, 4, 1024)\n",
    "        upsample(256, 4), # (bs, 8, 8, 1024)\n",
    "        upsample(128, 4), # (bs, 16, 16, 1024)\n",
    "        upsample(64, 4), # (bs, 32, 32, 512)\n",
    "    ]\n",
    "\n",
    "    initializer     = tf.keras.initializers.he_normal(seed=None)    \n",
    "    OUTPUT_CHANNELS = 1\n",
    "    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh') # (bs, 64, 64, 1)\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x_1 = down(x_1)\n",
    "        skips.append(x_1)\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x_1 = up(x_1)\n",
    "        x_1 = concat([x_1, skip])\n",
    "\n",
    "    x_1 = last(x_1) \n",
    "    \n",
    "    return x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 7: NIR2RGB network\n",
    "\n",
    "def NIR2RGB_convertor(input_x):\n",
    "    # input shape:  (64, 64, 1)\n",
    "    # output shape: (64, 64, 3)\n",
    "    \n",
    "    x_1 = input_x\n",
    "    \n",
    "    down_stack = [\n",
    "        downsample(64, 4,  apply_batchnorm=False), # (bs, 32, 32, 64)\n",
    "        downsample(128, 4, apply_batchnorm=True),  # (bs, 16, 16, 128)\n",
    "        downsample(256, 4, apply_batchnorm=True),  # (bs, 8, 8, 256)\n",
    "        downsample(256, 4, apply_batchnorm=True),  # (bs, 4, 4, 256)\n",
    "        downsample(256, 4, apply_batchnorm=True),  # (bs, 2, 2, 256)\n",
    "        downsample(256, 4, apply_batchnorm=True),  # (bs, 1, 1, 256)\n",
    "    ]\n",
    "    '''\n",
    "    up_stack = [\n",
    "        upsample(256, 4, apply_dropout=True), # (bs, 2, 2, 256)\n",
    "        upsample(256, 4, apply_dropout=True), # (bs, 4, 4, 256)\n",
    "        upsample(256, 4, apply_dropout=True), # (bs, 8, 8, 256)\n",
    "        upsample(128, 4, apply_dropout=True), # (bs, 16, 16, 128)\n",
    "        upsample(64,  4, apply_dropout=False), # (bs, 32, 32, 64)\n",
    "    ]\n",
    "    '''\n",
    "    up_stack = [\n",
    "        upsample(256, 4), # (bs, 2, 2, 256)\n",
    "        upsample(256, 4), # (bs, 4, 4, 256)\n",
    "        upsample(256, 4), # (bs, 8, 8, 256)\n",
    "        upsample(128, 4), # (bs, 16, 16, 128)\n",
    "        upsample(64,  4), # (bs, 32, 32, 64)\n",
    "    ]\n",
    "    \n",
    "    initializer     = tf.keras.initializers.he_normal(seed=None)    \n",
    "    OUTPUT_CHANNELS = 3\n",
    "    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh') # (bs, 64, 64, 1)\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x_1 = down(x_1)\n",
    "        skips.append(x_1)\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x_1 = up(x_1)\n",
    "        x_1 = concat([x_1, skip])\n",
    "\n",
    "    x_1 = last(x_1) \n",
    "    \n",
    "    return x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 8: NIR domain matching\n",
    "\n",
    "def NIR_domain_matching(input_x1, input_x2):\n",
    "    x_1 = input_x1\n",
    "    x_2 = input_x2\n",
    "    \n",
    "    # for x_1\n",
    "    layer1 = extract_first_features(32, 3, 1, True)\n",
    "    layer2 = extract_first_features(64, 3, 1, True)\n",
    "    layer3 = extract_first_features(128, 3, 1, True)\n",
    "    layer4 = extract_first_features(128, 5, 2, True)\n",
    "    layer5 = extract_first_features(256, 3, 1, True)\n",
    "    layer6 = extract_first_features(256, 5, 2, True)\n",
    "    layer7 = extract_first_features(256, 3, 1, True)\n",
    "    layer8 = extract_first_features(256, 5, 2, True)\n",
    "            \n",
    "    # for x_1\n",
    "    x_1 = layer1(x_1)\n",
    "    x_1 = layer2(x_1)\n",
    "    x_1 = layer3(x_1)\n",
    "    x_1 = layer4(x_1)\n",
    "    x_1 = layer5(x_1)\n",
    "    x_1 = layer6(x_1)\n",
    "    x_1 = layer7(x_1)\n",
    "    x_1 = layer8(x_1)\n",
    "    x_1 = layers.Flatten()(x_1)\n",
    "    \n",
    "    # for x_2\n",
    "    x_2 = layer1(x_2)\n",
    "    x_2 = layer2(x_2)\n",
    "    x_2 = layer3(x_2)\n",
    "    x_2 = layer4(x_2)\n",
    "    x_2 = layer5(x_2)\n",
    "    x_2 = layer6(x_2)\n",
    "    x_2 = layer7(x_2)\n",
    "    x_2 = layer8(x_2)\n",
    "    x_2 = layers.Flatten()(x_2)    \n",
    "    \n",
    "    x = tf.abs(x_1-x_2)\n",
    "    x = tf.concat([x_1, x_2, x], 1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 9: RGB domain matching\n",
    "\n",
    "def RGB_domain_matching(input_x1, input_x2):\n",
    "    x_1 = input_x1\n",
    "    x_2 = input_x2\n",
    "    \n",
    "    # for x_1\n",
    "    layer1 = extract_first_features(32, 3, 1, True)\n",
    "    layer2 = extract_first_features(64, 3, 1, True)\n",
    "    layer3 = extract_first_features(128, 3, 1, True)\n",
    "    layer4 = extract_first_features(128, 5, 2, True)\n",
    "    layer5 = extract_first_features(256, 3, 1, True)\n",
    "    layer6 = extract_first_features(256, 5, 2, True)\n",
    "    layer7 = extract_first_features(256, 3, 1, True)\n",
    "    layer8 = extract_first_features(256, 5, 2, True)\n",
    "            \n",
    "    # for x_1\n",
    "    x_1 = layer1(x_1)\n",
    "    x_1 = layer2(x_1)\n",
    "    x_1 = layer3(x_1)\n",
    "    x_1 = layer4(x_1)\n",
    "    x_1 = layer5(x_1)\n",
    "    x_1 = layer6(x_1)\n",
    "    x_1 = layer7(x_1)\n",
    "    x_1 = layer8(x_1)\n",
    "    x_1 = layers.Flatten()(x_1)\n",
    "    \n",
    "    # for x_2\n",
    "    x_2 = layer1(x_2)\n",
    "    x_2 = layer2(x_2)\n",
    "    x_2 = layer3(x_2)\n",
    "    x_2 = layer4(x_2)\n",
    "    x_2 = layer5(x_2)\n",
    "    x_2 = layer6(x_2)\n",
    "    x_2 = layer7(x_2)\n",
    "    x_2 = layer8(x_2)\n",
    "    x_2 = layers.Flatten()(x_2)    \n",
    "    \n",
    "    x = tf.abs(x_1-x_2)\n",
    "    x = tf.concat([x_1, x_2, x], 1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 10: construct SPIMNet network\n",
    "\n",
    "def make_similarity_model():      \n",
    "    inputs_1 = layers.Input(shape=[64, 64, 3])\n",
    "    inputs_2 = layers.Input(shape=[64, 64, 1])\n",
    "    x_rgb = inputs_1\n",
    "    x_nir = inputs_2\n",
    "    \n",
    "    # convert domains  \n",
    "    x_converted_nir = RGB2NIR_convertor(x_rgb)\n",
    "    x_converted_rgb = NIR2RGB_convertor(x_nir)\n",
    "    \n",
    "    # matching\n",
    "    f_nir = NIR_domain_matching(x_nir, x_converted_nir)\n",
    "    f_rgb = RGB_domain_matching(x_rgb, x_converted_rgb)\n",
    "    \n",
    "    # concat features\n",
    "    x = tf.concat([f_nir, f_rgb], 1)\n",
    "    \n",
    "    # metric learning\n",
    "    x = layers.Dense(1024)(x)\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[inputs_1, inputs_2], outputs=[x, x_rgb, x_converted_rgb, x_nir, x_converted_nir])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 32, 32, 64)   3072        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 32, 32, 64)   1024        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 16, 16, 128)  131840      sequential[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 16, 16, 128)  131840      sequential_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 8, 8, 256)    525824      sequential_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sequential_13 (Sequential)      (None, 8, 8, 256)    525824      sequential_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 4, 4, 256)    1050112     sequential_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sequential_14 (Sequential)      (None, 4, 4, 256)    1050112     sequential_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 2, 2, 256)    1050112     sequential_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sequential_15 (Sequential)      (None, 2, 2, 256)    1050112     sequential_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 1, 1, 256)    1050112     sequential_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sequential_16 (Sequential)      (None, 1, 1, 256)    1050112     sequential_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 2, 2, 256)    1050112     sequential_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sequential_17 (Sequential)      (None, 2, 2, 256)    1050112     sequential_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       multiple             0           sequential_6[0][0]               \n",
      "                                                                 sequential_4[0][0]               \n",
      "                                                                 sequential_7[0][0]               \n",
      "                                                                 sequential_3[0][0]               \n",
      "                                                                 sequential_8[0][0]               \n",
      "                                                                 sequential_2[0][0]               \n",
      "                                                                 sequential_9[0][0]               \n",
      "                                                                 sequential_1[0][0]               \n",
      "                                                                 sequential_10[0][0]              \n",
      "                                                                 sequential[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     multiple             0           sequential_17[0][0]              \n",
      "                                                                 sequential_15[0][0]              \n",
      "                                                                 sequential_18[0][0]              \n",
      "                                                                 sequential_14[0][0]              \n",
      "                                                                 sequential_19[0][0]              \n",
      "                                                                 sequential_13[0][0]              \n",
      "                                                                 sequential_20[0][0]              \n",
      "                                                                 sequential_12[0][0]              \n",
      "                                                                 sequential_21[0][0]              \n",
      "                                                                 sequential_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 4, 4, 256)    2098688     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_18 (Sequential)      (None, 4, 4, 256)    2098688     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_8 (Sequential)       (None, 8, 8, 256)    2098688     concatenate[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_19 (Sequential)      (None, 8, 8, 256)    2098688     concatenate_1[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16, 16, 128)  1049344     concatenate[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_20 (Sequential)      (None, 16, 16, 128)  1049344     concatenate_1[2][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 32, 32, 64)   262528      concatenate[3][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_21 (Sequential)      (None, 32, 32, 64)   262528      concatenate_1[3][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 64, 64, 1)    2049        concatenate[4][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_11 (Conv2DTran (None, 64, 64, 3)    6147        concatenate_1[4][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_22 (Sequential)      (None, 64, 64, 32)   480         input_2[0][0]                    \n",
      "                                                                 conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "sequential_30 (Sequential)      (None, 64, 64, 32)   1056        input_1[0][0]                    \n",
      "                                                                 conv2d_transpose_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "sequential_23 (Sequential)      (None, 64, 64, 64)   18816       sequential_22[0][0]              \n",
      "                                                                 sequential_22[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_31 (Sequential)      (None, 64, 64, 64)   18816       sequential_30[0][0]              \n",
      "                                                                 sequential_30[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_24 (Sequential)      (None, 64, 64, 128)  74496       sequential_23[0][0]              \n",
      "                                                                 sequential_23[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_32 (Sequential)      (None, 64, 64, 128)  74496       sequential_31[0][0]              \n",
      "                                                                 sequential_31[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_25 (Sequential)      (None, 32, 32, 128)  410368      sequential_24[0][0]              \n",
      "                                                                 sequential_24[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_33 (Sequential)      (None, 32, 32, 128)  410368      sequential_32[0][0]              \n",
      "                                                                 sequential_32[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_26 (Sequential)      (None, 32, 32, 256)  296448      sequential_25[0][0]              \n",
      "                                                                 sequential_25[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_34 (Sequential)      (None, 32, 32, 256)  296448      sequential_33[0][0]              \n",
      "                                                                 sequential_33[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_27 (Sequential)      (None, 16, 16, 256)  1639936     sequential_26[0][0]              \n",
      "                                                                 sequential_26[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_35 (Sequential)      (None, 16, 16, 256)  1639936     sequential_34[0][0]              \n",
      "                                                                 sequential_34[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_28 (Sequential)      (None, 16, 16, 256)  591360      sequential_27[0][0]              \n",
      "                                                                 sequential_27[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_36 (Sequential)      (None, 16, 16, 256)  591360      sequential_35[0][0]              \n",
      "                                                                 sequential_35[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_29 (Sequential)      (None, 8, 8, 256)    1639936     sequential_28[0][0]              \n",
      "                                                                 sequential_28[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_37 (Sequential)      (None, 8, 8, 256)    1639936     sequential_36[0][0]              \n",
      "                                                                 sequential_36[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 16384)        0           sequential_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 16384)        0           sequential_29[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 16384)        0           sequential_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 16384)        0           sequential_37[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract (TFOpLambda)   (None, 16384)        0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_1 (TFOpLambda) (None, 16384)        0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.abs (TFOpLambda)        (None, 16384)        0           tf.math.subtract[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.abs_1 (TFOpLambda)      (None, 16384)        0           tf.math.subtract_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 49152)        0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 tf.math.abs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_1 (TFOpLambda)        (None, 49152)        0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 tf.math.abs_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_2 (TFOpLambda)        (None, 98304)        0           tf.concat[0][0]                  \n",
      "                                                                 tf.concat_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         100664320   tf.concat_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          131200      dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            129         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 130,886,917\n",
      "Trainable params: 130,872,965\n",
      "Non-trainable params: 13,952\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# cell 11: Build SPIMNet and print it\n",
    "\n",
    "similaritor = make_similarity_model()\n",
    "similaritor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 7s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# cell 12: Construct content loss (~perceptual loss)\n",
    "\n",
    "from tensorflow.python.keras.applications.vgg19 import VGG19\n",
    "\n",
    "def vgg_54():\n",
    "    return _vgg(20)\n",
    "\n",
    "def _vgg(output_layer):\n",
    "    vgg = VGG19(input_shape=(64, 64, 3), include_top=False, weights='imagenet')\n",
    "    return tf.keras.Model(vgg.input, vgg.layers[output_layer].output)\n",
    "\n",
    "vgg = vgg_54()\n",
    "\n",
    "mean_squared_error = tf.keras.losses.MeanSquaredError()\n",
    "def content_loss(img1, img2):    \n",
    "    img1_fea = vgg(img1)\n",
    "    img2_fea = vgg(img2)\n",
    "        \n",
    "    loss = mean_squared_error(img1_fea, img2_fea)\n",
    "            \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012737564\n"
     ]
    }
   ],
   "source": [
    "# test case\n",
    "img1 = tf.random.normal((1, 64, 64, 3))\n",
    "img2 = tf.random.normal((1, 64, 64, 3))\n",
    "cont_loss_t = content_loss(img1, img2)\n",
    "print(cont_loss_t.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 13: build loss function\n",
    "\n",
    "# path to save checkpoints\n",
    "checkpoint_dir    = ''\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint        = tf.train.Checkpoint(similaritor=similaritor)\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.01)\n",
    "\n",
    "beta = 30.0\n",
    "alpha = 0.1\n",
    "def similaritor_loss(pos_output, x_rgb, x_converted_rgb, x_nir, x_converted_nir, neg_output):\n",
    "    # total_loss1\n",
    "    pos_loss = cross_entropy(tf.ones_like(pos_output), pos_output)    \n",
    "    neg_loss = cross_entropy(tf.zeros_like(neg_output), neg_output)\n",
    "    total_loss1 = pos_loss + neg_loss\n",
    "    \n",
    "    # total_loss3\n",
    "    l1_loss1  = tf.reduce_mean(tf.abs(x_rgb - x_converted_rgb))\n",
    "    l1_loss2  = tf.reduce_mean(tf.abs(x_nir - x_converted_nir))\n",
    "    total_loss3  = alpha*l1_loss1 + alpha*l1_loss2    \n",
    "    \n",
    "    # total_loss2\n",
    "    pos_nir1 = tf.concat([x_nir, x_nir, x_nir], 3)\n",
    "    pos_nir2 = tf.concat([x_converted_nir, x_converted_nir, x_converted_nir], 3)\n",
    "    \n",
    "    pos_nir_loss = content_loss(pos_nir1, pos_nir2)\n",
    "    pos_rgb_loss = content_loss(x_rgb, x_converted_rgb)\n",
    "    total_loss2  = pos_nir_loss*beta + pos_rgb_loss*beta\n",
    "    \n",
    "    # total_loss\n",
    "    total_loss = total_loss1 + total_loss2 + total_loss3\n",
    "    return total_loss, total_loss1, total_loss2, total_loss3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 14: train SPIMNet\n",
    "\n",
    "def train(train_data, epochs):\n",
    "    for epoch in range(1, epochs):\n",
    "        \n",
    "        # learning rate\n",
    "        if epoch < 20:\n",
    "            lr = 1e-4\n",
    "        else:\n",
    "            lr = 1e-5\n",
    "        optimizer = tf.keras.optimizers.Adam(lr)        \n",
    "        \n",
    "        average_loss = 0\n",
    "        average_posl = 0\n",
    "        average_negl = 0\n",
    "        average_l1lo = 0\n",
    "                \n",
    "        count = 0        \n",
    "        count_ones_pos = 0\n",
    "        count_ones_neg = 0\n",
    "    \n",
    "        for pos_bs_img0, pos_bs_img1, neg_bs_img0, neg_bs_img1 in train_data:  \n",
    "            pos_bs_img1 = pos_bs_img1[:,:,:,0:1]\n",
    "            neg_bs_img1 = neg_bs_img1[:,:,:,0:1]\n",
    "            \n",
    "            with tf.GradientTape() as sim_tape:  \n",
    "                # training\n",
    "                pos_output,x_rgb, x_c_rgb, x_nir, x_c_nir = similaritor([pos_bs_img0, pos_bs_img1], training=True)\n",
    "                neg_output, _, _, _, _ = similaritor([neg_bs_img0, neg_bs_img1], training=True)\n",
    "                \n",
    "                sim_loss, pos_loss, neg_loss, total_loss3 = similaritor_loss(pos_output,x_rgb, x_c_rgb, x_nir, x_c_nir, neg_output)\n",
    "\n",
    "                # --------- compute training acc ---------\n",
    "                bool_pos_output = pos_output > 0\n",
    "                ones_pos_output = tf.reduce_sum(tf.cast(bool_pos_output, tf.float32))        \n",
    "                count_ones_pos  = count_ones_pos + ones_pos_output\n",
    "\n",
    "                bool_neg_output = neg_output < 0\n",
    "                ones_neg_output = tf.reduce_sum(tf.cast(bool_neg_output, tf.float32))        \n",
    "                count_ones_neg  = count_ones_neg + ones_neg_output\n",
    "            \n",
    "            gradients = sim_tape.gradient(sim_loss, similaritor.trainable_variables)            \n",
    "            optimizer.apply_gradients(zip(gradients, similaritor.trainable_variables))\n",
    "            \n",
    "            average_loss = average_loss + sim_loss\n",
    "            average_posl = average_posl + pos_loss\n",
    "            average_negl = average_negl + neg_loss\n",
    "            average_l1lo = average_l1lo + total_loss3\n",
    "                        \n",
    "            count = count + 1\n",
    "            \n",
    "        average_loss = average_loss / count\n",
    "        average_posl = average_posl / count\n",
    "        average_negl = average_negl / count\n",
    "        average_l1lo = average_l1lo / count\n",
    "                \n",
    "        print('epoch {}  average_loss {}  lr {}'.format(epoch, average_loss, lr)) \n",
    "        print('normal loss {}  perceptual loss {}  l1 loss {}'.format(average_posl, average_negl, average_l1lo))  \n",
    "        \n",
    "        pos_acc = (count_ones_pos*100.0) / n_train_samples\n",
    "        neg_acc = (count_ones_neg*100.0) / n_train_samples\n",
    "        print('train acc (pos) {} - acc (neg) {}'.format(pos_acc, neg_acc))\n",
    "        \n",
    "        if epoch%10 == 0:            \n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            compute_test_acc()\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute FPR95 for testing sets\n",
    "\n",
    "def compute_test_acc():\n",
    "    path_test  = '/home/shared_dir/research/SPIMNet/NIR_RGB/'\n",
    "    cate_names = ['field', 'indoor', 'oldbuilding', 'street', 'urban', 'water', 'forest', 'mountain']\n",
    "    cate_index = [120448, 30336, 50688, 82304, 73856, 71552, 188416, 75648]\n",
    "\n",
    "    for i in range(0,8):\n",
    "        test_dataset = tf.data.Dataset.list_files(path_test + 'patch-merged-datasets/' + cate_names[i] + '/*.jpg')\n",
    "        test_dataset = test_dataset.map(load_image_test)\n",
    "        test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "        print(cate_names[i] + ' ...')\n",
    "\n",
    "        scores_pos = []\n",
    "        scores_neg = []\n",
    "\n",
    "        for pos_bs_img0, pos_bs_img1, neg_bs_img0, neg_bs_img1 in test_dataset:\n",
    "            pos_bs_img1 = pos_bs_img1[:,:,:,0:1]\n",
    "            neg_bs_img1 = neg_bs_img1[:,:,:,0:1]\n",
    "\n",
    "            data_outputp, _, _, _, _ = similaritor([pos_bs_img0, pos_bs_img1], training=True)\n",
    "            data_outputn, _, _, _, _ = similaritor([neg_bs_img0, neg_bs_img1], training=True)\n",
    "\n",
    "            s_pos = tf.math.sigmoid(data_outputp)\n",
    "            s_neg = tf.math.sigmoid(data_outputn)\n",
    "\n",
    "            scores_pos.append(s_pos[:,0].numpy())\n",
    "            scores_neg.append(s_neg[:,0].numpy())\n",
    "\n",
    "        scores_np_pos = np.concatenate(scores_pos, axis=0)\n",
    "        scores_np_neg = np.concatenate(scores_neg, axis=0)\n",
    "\n",
    "        labels_pos = np.ones((cate_index[i],), dtype=int)\n",
    "        labels_neg = np.zeros((cate_index[i],), dtype=int)\n",
    "        \n",
    "        scores_np = np.concatenate((scores_np_pos,scores_np_neg), axis=0)\n",
    "        labels_np = np.concatenate((labels_pos,labels_neg), axis=0)\n",
    "\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(labels_np, scores_np, pos_label=1)\n",
    "        fpr95 = float(interpolate.interp1d(tpr, fpr)(0.95))\n",
    "        print('FPR95:', fpr95)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  average_loss 2.962709426879883  lr 0.0001\n",
      "normal loss 0.5010076761245728  perceptual loss 2.3033647537231445  l1 loss 0.1583368480205536\n",
      "train acc (pos) 91.44445037841797 - acc (neg) 93.17631530761719\n",
      "\n",
      "epoch 2  average_loss 2.1625747680664062  lr 0.0001\n",
      "normal loss 0.22761917114257812  perceptual loss 1.799654245376587  l1 loss 0.13529269397258759\n",
      "train acc (pos) 96.71932983398438 - acc (neg) 97.79029846191406\n",
      "\n",
      "epoch 3  average_loss 1.9807138442993164  lr 0.0001\n",
      "normal loss 0.17784222960472107  perceptual loss 1.687182903289795  l1 loss 0.11569993942975998\n",
      "train acc (pos) 97.72111511230469 - acc (neg) 98.46920776367188\n",
      "\n",
      "epoch 4  average_loss 1.8795654773712158  lr 0.0001\n",
      "normal loss 0.15127530694007874  perceptual loss 1.626526117324829  l1 loss 0.10176645964384079\n",
      "train acc (pos) 98.3113784790039 - acc (neg) 98.87208557128906\n",
      "\n",
      "epoch 5  average_loss 1.8101754188537598  lr 0.0001\n",
      "normal loss 0.13693860173225403  perceptual loss 1.5802676677703857  l1 loss 0.09296322613954544\n",
      "train acc (pos) 98.7020034790039 - acc (neg) 99.0969467163086\n",
      "\n",
      "epoch 6  average_loss 1.7602534294128418  lr 0.0001\n",
      "normal loss 0.1281254142522812  perceptual loss 1.5450828075408936  l1 loss 0.08704951405525208\n",
      "train acc (pos) 98.92542266845703 - acc (neg) 99.22451782226562\n",
      "\n",
      "epoch 7  average_loss 1.7165586948394775  lr 0.0001\n",
      "normal loss 0.1225026547908783  perceptual loss 1.5115731954574585  l1 loss 0.08248329162597656\n",
      "train acc (pos) 99.09550476074219 - acc (neg) 99.27784729003906\n",
      "\n",
      "epoch 8  average_loss 1.6812584400177002  lr 0.0001\n",
      "normal loss 0.11664614826440811  perceptual loss 1.4858198165893555  l1 loss 0.07878893613815308\n",
      "train acc (pos) 99.23316192626953 - acc (neg) 99.3751449584961\n",
      "\n",
      "epoch 9  average_loss 1.6479359865188599  lr 0.0001\n",
      "normal loss 0.11228214204311371  perceptual loss 1.4600976705551147  l1 loss 0.07555344700813293\n",
      "train acc (pos) 99.38451385498047 - acc (neg) 99.42919921875\n",
      "\n",
      "epoch 10  average_loss 1.6207306385040283  lr 0.0001\n",
      "normal loss 0.1089773178100586  perceptual loss 1.4388444423675537  l1 loss 0.072904571890831\n",
      "train acc (pos) 99.4529800415039 - acc (neg) 99.5178451538086\n",
      "field ...\n",
      "FPR95: 0.025288921360255047\n",
      "\n",
      "indoor ...\n",
      "FPR95: 0.027374805685098813\n",
      "\n",
      "oldbuilding ...\n",
      "FPR95: 0.009574258207070701\n",
      "\n",
      "street ...\n",
      "FPR95: 0.0034317894634525556\n",
      "\n",
      "urban ...\n",
      "FPR95: 0.0066968154246100295\n",
      "\n",
      "water ...\n",
      "FPR95: 0.025422070661896244\n",
      "\n",
      "forest ...\n",
      "FPR95: 0.0014064622961956522\n",
      "\n",
      "mountain ...\n",
      "FPR95: 0.010876692047377304\n",
      "\n",
      "\n",
      "epoch 11  average_loss 1.5928925275802612  lr 0.0001\n",
      "normal loss 0.10580170899629593  perceptual loss 1.416292667388916  l1 loss 0.07079625874757767\n",
      "train acc (pos) 99.52288818359375 - acc (neg) 99.54523468017578\n",
      "\n",
      "epoch 12  average_loss 1.5701322555541992  lr 0.0001\n",
      "normal loss 0.10327421873807907  perceptual loss 1.3979377746582031  l1 loss 0.06891504675149918\n",
      "train acc (pos) 99.60649108886719 - acc (neg) 99.56685638427734\n",
      "\n",
      "epoch 13  average_loss 1.5480695962905884  lr 0.0001\n",
      "normal loss 0.10061237961053848  perceptual loss 1.3800430297851562  l1 loss 0.06740883737802505\n",
      "train acc (pos) 99.66847229003906 - acc (neg) 99.62162780761719\n",
      "\n",
      "epoch 14  average_loss 1.5265498161315918  lr 0.0001\n",
      "normal loss 0.09836207330226898  perceptual loss 1.3620216846466064  l1 loss 0.06616077572107315\n",
      "train acc (pos) 99.68865203857422 - acc (neg) 99.64901733398438\n",
      "\n",
      "epoch 15  average_loss 1.5072057247161865  lr 0.0001\n",
      "normal loss 0.09623505920171738  perceptual loss 1.3460215330123901  l1 loss 0.06494605541229248\n",
      "train acc (pos) 99.75567626953125 - acc (neg) 99.6785659790039\n",
      "\n",
      "epoch 16  average_loss 1.4888527393341064  lr 0.0001\n",
      "normal loss 0.09366612881422043  perceptual loss 1.3311837911605835  l1 loss 0.06399641931056976\n",
      "train acc (pos) 99.78162384033203 - acc (neg) 99.70667266845703\n",
      "\n",
      "epoch 17  average_loss 1.4728443622589111  lr 0.0001\n",
      "normal loss 0.09144546836614609  perceptual loss 1.318354606628418  l1 loss 0.06304933875799179\n",
      "train acc (pos) 99.81549835205078 - acc (neg) 99.77009582519531\n",
      "\n",
      "epoch 18  average_loss 1.4558002948760986  lr 0.0001\n",
      "normal loss 0.08996085822582245  perceptual loss 1.3034991025924683  l1 loss 0.062328241765499115\n",
      "train acc (pos) 99.81621551513672 - acc (neg) 99.78306579589844\n",
      "\n",
      "epoch 19  average_loss 1.4409099817276  lr 0.0001\n",
      "normal loss 0.08854479342699051  perceptual loss 1.2907018661499023  l1 loss 0.06166568025946617\n",
      "train acc (pos) 99.84288787841797 - acc (neg) 99.80757141113281\n",
      "\n",
      "epoch 20  average_loss 1.3823692798614502  lr 1e-05\n",
      "normal loss 0.08198296278715134  perceptual loss 1.2396531105041504  l1 loss 0.060726575553417206\n",
      "train acc (pos) 99.93873596191406 - acc (neg) 99.87892150878906\n",
      "field ...\n",
      "FPR95: 0.022972569075451647\n",
      "\n",
      "indoor ...\n",
      "FPR95: 0.01753691983122363\n",
      "\n",
      "oldbuilding ...\n",
      "FPR95: 0.006901830808080783\n",
      "\n",
      "street ...\n",
      "FPR95: 0.0031502721617418347\n",
      "\n",
      "urban ...\n",
      "FPR95: 0.0037640814558058924\n",
      "\n",
      "water ...\n",
      "FPR95: 0.023689065295169946\n",
      "\n",
      "forest ...\n",
      "FPR95: 0.0011355227228613777\n",
      "\n",
      "mountain ...\n",
      "FPR95: 0.009846481951494627\n",
      "\n",
      "\n",
      "epoch 21  average_loss 1.367266297340393  lr 1e-05\n",
      "normal loss 0.08008747547864914  perceptual loss 1.2268691062927246  l1 loss 0.06031499430537224\n",
      "train acc (pos) 99.96685028076172 - acc (neg) 99.90486907958984\n",
      "\n",
      "epoch 22  average_loss 1.3596773147583008  lr 1e-05\n",
      "normal loss 0.07938598096370697  perceptual loss 1.2201496362686157  l1 loss 0.06014424189925194\n",
      "train acc (pos) 99.96900939941406 - acc (neg) 99.91279602050781\n",
      "\n",
      "epoch 23  average_loss 1.3552073240280151  lr 1e-05\n",
      "normal loss 0.07885628938674927  perceptual loss 1.2163798809051514  l1 loss 0.05997643619775772\n",
      "train acc (pos) 99.97477722167969 - acc (neg) 99.925048828125\n",
      "\n",
      "epoch 24  average_loss 1.3509340286254883  lr 1e-05\n",
      "normal loss 0.0786399096250534  perceptual loss 1.2124770879745483  l1 loss 0.05981527641415596\n",
      "train acc (pos) 99.9711685180664 - acc (neg) 99.91999816894531\n",
      "\n",
      "epoch 25  average_loss 1.3466417789459229  lr 1e-05\n",
      "normal loss 0.0782494843006134  perceptual loss 1.208670973777771  l1 loss 0.05972312390804291\n",
      "train acc (pos) 99.9711685180664 - acc (neg) 99.92216491699219\n",
      "\n",
      "epoch 26  average_loss 1.342436671257019  lr 1e-05\n",
      "normal loss 0.07788750529289246  perceptual loss 1.2049225568771362  l1 loss 0.05962507054209709\n",
      "train acc (pos) 99.9791030883789 - acc (neg) 99.93153381347656\n",
      "\n",
      "epoch 27  average_loss 1.3373510837554932  lr 1e-05\n",
      "normal loss 0.0774063766002655  perceptual loss 1.2004339694976807  l1 loss 0.05951172485947609\n",
      "train acc (pos) 99.98270416259766 - acc (neg) 99.94017791748047\n",
      "\n",
      "epoch 28  average_loss 1.334946870803833  lr 1e-05\n",
      "normal loss 0.07719572633504868  perceptual loss 1.198350429534912  l1 loss 0.0593978688120842\n",
      "train acc (pos) 99.9863052368164 - acc (neg) 99.93585968017578\n",
      "\n",
      "epoch 29  average_loss 1.3319560289382935  lr 1e-05\n",
      "normal loss 0.07704032212495804  perceptual loss 1.1955997943878174  l1 loss 0.059317696839571\n",
      "train acc (pos) 99.98053741455078 - acc (neg) 99.94161987304688\n",
      "\n",
      "epoch 30  average_loss 1.329081416130066  lr 1e-05\n",
      "normal loss 0.07674850523471832  perceptual loss 1.1931101083755493  l1 loss 0.059220217168331146\n",
      "train acc (pos) 99.98558807373047 - acc (neg) 99.94522857666016\n",
      "field ...\n",
      "FPR95: 0.022778294367693935\n",
      "\n",
      "indoor ...\n",
      "FPR95: 0.016185390295358648\n",
      "\n",
      "oldbuilding ...\n",
      "FPR95: 0.006904987373737374\n",
      "\n",
      "street ...\n",
      "FPR95: 0.0029279257387247264\n",
      "\n",
      "urban ...\n",
      "FPR95: 0.004166092248306283\n",
      "\n",
      "water ...\n",
      "FPR95: 0.02259894901610018\n",
      "\n",
      "forest ...\n",
      "FPR95: 0.0009871773097826087\n",
      "\n",
      "mountain ...\n",
      "FPR95: 0.008843591370558375\n",
      "\n",
      "\n",
      "epoch 31  average_loss 1.3253259658813477  lr 1e-05\n",
      "normal loss 0.07665343582630157  perceptual loss 1.1895251274108887  l1 loss 0.05914529040455818\n",
      "train acc (pos) 99.9834213256836 - acc (neg) 99.94090270996094\n",
      "\n",
      "epoch 32  average_loss 1.3232905864715576  lr 1e-05\n",
      "normal loss 0.07626882195472717  perceptual loss 1.187952995300293  l1 loss 0.059065792709589005\n",
      "train acc (pos) 99.98414611816406 - acc (neg) 99.95315551757812\n",
      "\n",
      "epoch 33  average_loss 1.320141077041626  lr 1e-05\n",
      "normal loss 0.07618235051631927  perceptual loss 1.1849857568740845  l1 loss 0.05897149443626404\n",
      "train acc (pos) 99.98774719238281 - acc (neg) 99.95819854736328\n",
      "\n",
      "epoch 34  average_loss 1.3164212703704834  lr 1e-05\n",
      "normal loss 0.07590550929307938  perceptual loss 1.181640386581421  l1 loss 0.058880116790533066\n",
      "train acc (pos) 99.99063110351562 - acc (neg) 99.95819854736328\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35  average_loss 1.313899040222168  lr 1e-05\n",
      "normal loss 0.07579156756401062  perceptual loss 1.1793227195739746  l1 loss 0.05878268554806709\n",
      "train acc (pos) 99.98918914794922 - acc (neg) 99.95243072509766\n",
      "\n",
      "field ...\n",
      "FPR95: 0.023516372210414287\n",
      "\n",
      "indoor ...\n",
      "FPR95: 0.017728111814345967\n",
      "\n",
      "oldbuilding ...\n",
      "FPR95: 0.006944444444444444\n",
      "\n",
      "street ...\n",
      "FPR95: 0.0029038685847589426\n",
      "\n",
      "urban ...\n",
      "FPR95: 0.004265056325823223\n",
      "\n",
      "water ...\n",
      "FPR95: 0.02396019677996422\n",
      "\n",
      "forest ...\n",
      "FPR95: 0.0011092476222826087\n",
      "\n",
      "mountain ...\n",
      "FPR95: 0.009013677382966722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cell 15: train SPIMNet with 35 epochs\n",
    "\n",
    "EPOCHS = 36 # from 1\n",
    "train(train_dataset, EPOCHS)            \n",
    "checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "compute_test_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGC-DL-CONTAINER-LICENSE  dev\tlib    mnt   root  srv\ttmp\r\n",
      "bin\t\t\t  etc\tlib64  opt   run   sys\tusr\r\n",
      "boot\t\t\t  home\tmedia  proc  sbin  tf\tvar\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f763c64e0f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 16: restore the lastest checkpoint\n",
    "\n",
    "!ls {checkpoint_dir}/\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
